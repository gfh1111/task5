# -*- coding: utf-8 -*-
"""task2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hGNDb1_fbi78eSIMB2FoIQWFgFcb_ZFN
"""

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T
import sys
import csv

def step1(partId, rows):
    if partId==0:
        next(rows)
    import csv
    reader = csv.reader(rows)
    for row in reader:
        if len(row)>7 and type(row[0])==str and len(row[0])==10 and row[0][0]==('2'or'1'):
            yield (row[0][0:4],row[1].lower(),row[7].lower())
    
if __name__=="__main__":
    sc = pyspark.SparkContext()
    A = sc.textFile(sys.argv[1]).mapPartitionsWithIndex(step1)

    B = A.map(lambda x: ((x[1],x[0]),1))\
          .reduceByKey(lambda x,y: x+y)

    C = A.map(lambda x: ((x[1],x[0],x[2]),1))\
          .reduceByKey(lambda x,y: x+y)\
          .map(lambda x: ((x[0][0],x[0][1]),1))\
          .reduceByKey(lambda x,y: x+y)

    D = A.map(lambda x: ((x[1],x[0],x[2]),1))\
          .reduceByKey(lambda x,y: x+y)\
          .map(lambda x: ((x[0][0],x[0][1]),x[1]))\
          .reduceByKey(lambda x,y: max(x,y))

    E = B.join(C).join(D)\
          .map(lambda x: (x[0],(x[1][0][0],x[1][0][1],x[1][1])))\
          .map(lambda x: ((x[0][0],x[0][1]),(str(x[1][0]),str(x[1][1]),str(round(x[1][2]/x[1][0]*100)))))\
          .sortByKey()\
          .map(lambda x: x[0]+x[1])\
     
    outputTask2 = E.map(lambda x: ','.join(str(item) for item in x))
    outputTask2.saveAsTextFile(sys.argv[2])

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# gdown --quiet 1O1U_t-cpmValVK2mjdTzcFxIbGw05vOw
# gdown --quiet 1YUBKrtNV3QUz1RutMnMbJdQj7rv-Lkd5
# gdown --quiet 1f79oETtvN3NQLYPnVGhurE1UBDP4IQP-
# pip install pyspark
#

# Commented out IPython magic to ensure Python compatibility.
import csv
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import IPython
# %matplotlib inline
IPython.display.set_matplotlib_formats('svg')
pd.plotting.register_matplotlib_converters()
sns.set_style("whitegrid")

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T
sc = pyspark.SparkContext.getOrCreate()
spark = SparkSession(sc)
spark

## import dataset
 # a.keyfood_products.csv
KEYFOOD_FN = 'keyfood_products.csv'
 # b.keyfood_nyc_stores.json
NYCSTORES_FN  = pd.read_json("keyfood_nyc_stores.json",orient='index')
 # c.keyfood_sample_items.csv
ITEM_FN = 'keyfood_sample_items.csv'

## filter 22 items and extract price informations  
item_group = sc.textFile(ITEM_FN, use_unicode=True)\
               .filter(lambda x: not x.startswith('UPC code,Item Name'))\
               .map(lambda x: {x.split(',')[0].split('-')[1]})\
               .reduce(lambda x,y: x|y)
               
item_name = sc.textFile(ITEM_FN, use_unicode=True)\
               .filter(lambda x: not x.startswith('UPC code,Item Name'))\
               .map(lambda x: (x.split(',')[0].split('-')[1],x.split(',')[1]))

def extract(partId, rows):
    if partId==0:
        next(rows)
    reader = csv.reader(rows)
    for row in reader:
      if not row[2] == 'N/A':
        if row[2].split('-')[1] in item_group:
          (store, upc, price) = (row[0], row[2].split('-')[1], float(row[5].split()[0][1:]))
          yield (store, upc, price)

keyfood = sc.textFile(KEYFOOD_FN, use_unicode=True).cache()           
A = keyfood.mapPartitionsWithIndex(extract)\
           .map(lambda x: (x[1],(x[0],x[2])))\
           .join(item_name)\
           .map(lambda x: (x[1][0][0],(x[1][0][1],x[1][1])))

## extract foodinsecurity information 
store_group = A.map(lambda x: {x[0],1})\
               .reduce(lambda x,y: x|y)
nyu_stores = pd.DataFrame(data = NYCSTORES_FN , columns= ['name','communityDistrict','foodInsecurity'])
nyu_stores = spark.createDataFrame(nyu_stores)\
                  .rdd\
                  .map(lambda x:(str(x[0]),int(x[2]*100)))\
                  .filter(lambda x: x[0] in store_group)   

## combine all the information
outputTask1 = A.join(nyu_stores)\
               .map(lambda x: (x[1][0][1],x[1][0][0],x[1][1]))

## DO NOT EDIT BELOW
outputTask1 = outputTask1.cache()
outputTask1.count()
